<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear State Space Examples · DifferenceEquations.jl</title><meta name="title" content="Linear State Space Examples · DifferenceEquations.jl"/><meta property="og:title" content="Linear State Space Examples · DifferenceEquations.jl"/><meta property="twitter:title" content="Linear State Space Examples · DifferenceEquations.jl"/><meta name="description" content="Documentation for DifferenceEquations.jl."/><meta property="og:description" content="Documentation for DifferenceEquations.jl."/><meta property="twitter:description" content="Documentation for DifferenceEquations.jl."/><meta property="og:url" content="https://DifferenceEquations.sciml.ai/stable/examples/linear_state_space_examples/"/><meta property="twitter:url" content="https://DifferenceEquations.sciml.ai/stable/examples/linear_state_space_examples/"/><link rel="canonical" href="https://DifferenceEquations.sciml.ai/stable/examples/linear_state_space_examples/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DifferenceEquations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DifferenceEquations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">DifferenceEquations.jl: Discrete-Time State Space Solution Methods</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Linear State Space Examples</a><ul class="internal"><li><a class="tocitem" href="#Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model"><span>Simulating a Linear (and Time-Invariant) State Space Model</span></a></li><li><a class="tocitem" href="#Simulating-Ensembles-and-Fixing-Noise"><span>Simulating Ensembles and Fixing Noise</span></a></li><li><a class="tocitem" href="#Observables-and-Marginal-Likelihood-using-a-Kalman-Filter"><span>Observables and Marginal Likelihood using a Kalman Filter</span></a></li><li><a class="tocitem" href="#Joint-Likelihood-with-Noise"><span>Joint Likelihood with Noise</span></a></li><li><a class="tocitem" href="#Composition-of-State-Space-Models-and-AD"><span>Composition of State Space Models and AD</span></a></li><li><a class="tocitem" href="#Caveats-on-Gradients-and-Performance"><span>Caveats on Gradients and Performance</span></a></li></ul></li><li><a class="tocitem" href="../quadratic_state_space_examples/">Quadratic State Space Examples</a></li><li><a class="tocitem" href="../general_state_space_examples/">General State Space Examples</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Linear State Space Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear State Space Examples</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/DifferenceEquations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/DifferenceEquations.jl/blob/main/docs/src/examples/linear_state_space_examples.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-State-Space-Examples"><a class="docs-heading-anchor" href="#Linear-State-Space-Examples">Linear State Space Examples</a><a id="Linear-State-Space-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-State-Space-Examples" title="Permalink"></a></h1><p>This tutorial describes the support for linear and linear gaussian state space models.</p><p>At this point, the package only supports linear time-invariant models without a separate <code>p</code> vector. The canonical form of the linear model is</p><p class="math-container">\[u_{n+1} = A u_n + B w_{n+1}\]</p><p>with</p><p class="math-container">\[z_n = C u_n +  v_n\]</p><p>and optionally <span>$v_n \sim N(0, D)$</span> and <span>$w_{n+1} \sim N(0,I)$</span>.  If you pass noise into the solver, it no longer needs to be Gaussian. More generally, support could be added for <span>$u_{n+1} = A(p,n) u_n + B(p,n) w_{n+1}$</span> where <span>$p$</span> is a vector of differentiable parameters, and the <span>$A$</span> and <span>$B$</span> are potentially matrix-free operators.</p><h2 id="Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model"><a class="docs-heading-anchor" href="#Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model">Simulating a Linear (and Time-Invariant) State Space Model</a><a id="Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model" title="Permalink"></a></h2><p>Creating a <code>LinearStateSpaceProblem</code> and simulating it for a simple, linear equation.</p><pre><code class="language-julia hljs">using DifferenceEquations, LinearAlgebra, Distributions, Random, Plots, DataFrames, Zygote
A = [0.95 6.2;
     0.0  0.2]
B = [0.0; 0.01;;] # matrix
C = [0.09 0.67;
     1.00 0.00]
D = [0.1, 0.1] # diagonal observation noise
u0 = zeros(2)
T = 10

prob = LinearStateSpaceProblem(A, B, u0, (0, T); C, observables_noise = D, syms = [:a, :b])
sol = solve(prob)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
Interpolation: Piecewise constant interpolation
t: 0:10
u: 11-element Vector{Vector{Float64}}:
 [0.0, 0.0]
 [0.0, 0.008216615565688808]
 [0.050943016507270615, -0.0003717541508642362]
 [0.04609098994654882, 0.002839549525455006]
 [0.06139164750704241, -0.0032650031206296133]
 [0.03807904578378668, 0.0021420895848564153]
 [0.049456048920707124, 0.012071742976811316]
 [0.12182805293090193, -0.00014118434736562404]
 [0.11486130733068996, 0.00440881196550904]
 [0.13645287615031151, -0.007017314874542747]
 [0.0861228801206309, 0.010322195572103099]</code></pre><p>The <code>u</code> vector of the simulated solution can be plotted using the standard recipes, including the use of the optional <code>syms</code>. See the <a href="https://diffeq.sciml.ai/latest/basics/plot/">SciML docs</a> for more options.</p><pre><code class="language-julia hljs">plot(sol)</code></pre><img src="9f9c88aa.svg" alt="Example block output"/><p>By default, the solution provides an interface to access the simulated <code>u</code>.  That is, <code>sol.u[...] = sol[...]</code>,</p><pre><code class="language-julia hljs">sol[2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.0
 0.008216615565688808</code></pre><p>Or to get the first element of the last step</p><pre><code class="language-julia hljs">sol[end][1] #first element of last step</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0861228801206309</code></pre><p>Finally, to extract the full vector</p><pre><code class="language-julia hljs">@show sol[2,:];  # whole second vector</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11-element Vector{Float64}:
  0.0
  0.008216615565688808
 -0.0003717541508642362
  0.002839549525455006
 -0.0032650031206296133
  0.0021420895848564153
  0.012071742976811316
 -0.00014118434736562404
  0.00440881196550904
 -0.007017314874542747
  0.010322195572103099</code></pre><p>The results for all of <code>sol.u</code> can be loaded in a dataframe, where the column names will be the (optionally) provided symbols.</p><pre><code class="language-julia hljs">df = DataFrame(sol)</code></pre><div><div style = "float: left;"><span>11×3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">timestamp</th><th style = "text-align: left;">a</th><th style = "text-align: left;">b</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">0.0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">0.00821662</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">2</td><td style = "text-align: right;">0.050943</td><td style = "text-align: right;">-0.000371754</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">3</td><td style = "text-align: right;">0.046091</td><td style = "text-align: right;">0.00283955</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">4</td><td style = "text-align: right;">0.0613916</td><td style = "text-align: right;">-0.003265</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">6</td><td style = "text-align: right;">5</td><td style = "text-align: right;">0.038079</td><td style = "text-align: right;">0.00214209</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">7</td><td style = "text-align: right;">6</td><td style = "text-align: right;">0.049456</td><td style = "text-align: right;">0.0120717</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">8</td><td style = "text-align: right;">7</td><td style = "text-align: right;">0.121828</td><td style = "text-align: right;">-0.000141184</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">9</td><td style = "text-align: right;">8</td><td style = "text-align: right;">0.114861</td><td style = "text-align: right;">0.00440881</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">10</td><td style = "text-align: right;">9</td><td style = "text-align: right;">0.136453</td><td style = "text-align: right;">-0.00701731</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">11</td><td style = "text-align: right;">10</td><td style = "text-align: right;">0.0861229</td><td style = "text-align: right;">0.0103222</td></tr></tbody></table></div><p>Other results, such as the simulated noise and observables, can be extracted from the solution</p><pre><code class="language-julia hljs">sol.z # observables</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11-element Vector{Vector{Float64}}:
 [0.16552507233217995, 0.16866424995117757]
 [-0.011255779747774825, -0.03270922027837342]
 [-0.006848138502089897, 0.20116650148909315]
 [-0.2814924255950229, -0.24763934627351114]
 [-0.5112059464293531, -0.03180446721949314]
 [-0.1985970381051543, -0.22297977688998882]
 [0.18514411275340842, -0.7355527240839188]
 [0.12349044552598992, -0.2853283947698296]
 [0.39238410792567224, -0.03488617254839914]
 [-0.17570023882535718, 0.04445709846880734]
 [0.00145468427887133, -0.03349076570299714]</code></pre><pre><code class="language-julia hljs">sol.W # Simulated Noise</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×10 Matrix{Float64}:
 0.821662  -0.201508  0.29139  -0.383291  …  0.443705  -0.789908  1.17257</code></pre><p>We can also solve the model by passing in fixed noise, which will be useful for joint likelihoods. First, let&#39;s extract the noise from the previous solution, then rerun the simulation but with a different initial value</p><pre><code class="language-julia hljs">noise = sol.W
u0_2 = [0.1, 0.0]
prob2 = LinearStateSpaceProblem(A, B, u0_2, (0, T); C, observables_noise = D, syms = [:a, :b], noise)
sol2 = solve(prob2)
plot(sol2)</code></pre><img src="6b932be8.svg" alt="Example block output"/><p>To construct an IRF we can take the model and perturb just the first element of the noise,</p><pre><code class="language-julia hljs">function irf(A, B, C, T = 20)
    noise = Matrix([1.0; zeros(T-1)]&#39;)
    problem = LinearStateSpaceProblem(A, B, zeros(2), (0, T); C, noise, syms = [:a, :b])
    return solve(problem)
end
plot(irf(A, B, C))</code></pre><img src="4cd89f76.svg" alt="Example block output"/><p>Let&#39;s find the 2nd observable at the end of the IRF.</p><pre><code class="language-julia hljs">function last_observable_irf(A, B, C)
    sol = irf(A, B, C)
    return sol.z[end][2]  # return 2nd argument of last observable
end
last_observable_irf(A, B, C)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.03119456447624772</code></pre><p>But everything in this package is differentiable. Let&#39;s differentiate the observable of the IRF with respect to all the parameters using <code>Zygote.jl</code>,</p><pre><code class="language-julia hljs">gradient(last_observable_irf, A, B, C)  # calculates gradient wrt all arguments</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([0.5822985368900442 0.0050313813671367304; 4.469834483178462 0.041592752634585235], [0.37735360253530714; 3.119456447624773;;], [0.0 0.0; 0.03119456447624772 5.242880000000006e-16])</code></pre><p>Gradients of other model elements (e.g. <code>.u</code>) are also possible. With this in mind, let&#39;s find the gradient of the mean of the 1st element of the IRF of the solution with respect to a particular noise vector.</p><pre><code class="language-julia hljs">function mean_u_1(A, B, C, noise, u0, T)
    problem = LinearStateSpaceProblem(A, B, u0, (0, T); noise, syms = [:a, :b])
    sol = solve(problem)
    u = sol.u # see issue #75 workaround
    # can have nontrivial functions and even non-mutating loops
    return mean( u[i][1] for i in 1:T)
end
u0 = [0.0, 0.0]
noise = sol.W # from simulation above
mean_u_1(A, B, C, noise, u0, T)
# dropping a few arguments from derivative
gradient((noise, u0)-&gt; mean_u_1(A, B, C, noise, u0, T), noise, u0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([0.05079876954953124 0.045314515146875 … 0.0 0.0], [0.702526121523242, 5.600882710405467])</code></pre><h2 id="Simulating-Ensembles-and-Fixing-Noise"><a class="docs-heading-anchor" href="#Simulating-Ensembles-and-Fixing-Noise">Simulating Ensembles and Fixing Noise</a><a id="Simulating-Ensembles-and-Fixing-Noise-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-Ensembles-and-Fixing-Noise" title="Permalink"></a></h2><p>If you pass in a distribution for the initial condition, it will draw an initial condition. Below, we will simulate from a deterministic evolution equation, without any observation noise.</p><pre><code class="language-julia hljs">using Distributions, DiffEqBase
u0 = MvNormal([1.0 0.1; 0.1 1.0])  # mean zero initial conditions
prob = LinearStateSpaceProblem(A, nothing, u0, (0, T); C)
sol = solve(prob)
plot(sol)</code></pre><img src="7fbd3bfb.svg" alt="Example block output"/><p>With this, we can simulate an ensemble of solutions from different initial conditions (and we will turn back on the noise). The <code>EnsembleSummary</code> calculates a set of quantiles by default.</p><pre><code class="language-julia hljs">T = 10
trajectories = 50
prob = LinearStateSpaceProblem(A, B, u0, (0, T); C)
sol = solve(EnsembleProblem(prob), DirectIteration(), EnsembleThreads(); trajectories)
summ = EnsembleSummary(sol)  #calculate summarize statistics from the
plot(summ)  # shows quantiles by default</code></pre><img src="157b11d8.svg" alt="Example block output"/><h2 id="Observables-and-Marginal-Likelihood-using-a-Kalman-Filter"><a class="docs-heading-anchor" href="#Observables-and-Marginal-Likelihood-using-a-Kalman-Filter">Observables and Marginal Likelihood using a Kalman Filter</a><a id="Observables-and-Marginal-Likelihood-using-a-Kalman-Filter-1"></a><a class="docs-heading-anchor-permalink" href="#Observables-and-Marginal-Likelihood-using-a-Kalman-Filter" title="Permalink"></a></h2><p>If you provide <code>observables</code> and provide a distribution for the <code>observables_noise</code> then the model can provide a calculation of the likelihood.</p><p>The simplest case is if you use a gaussian prior and have gaussian observation noise. First, let&#39;s simulate some data with included observation noise. If passing in a matrix or vector, the <code>observables_noise</code> argument is intended to be the cholesky of the covariance matrix. At this point, only diagonal observation noise is allowed.</p><pre><code class="language-julia hljs">u0 = MvNormal([1.0 0.1; 0.1 1.0])  # draw from mean zero initial conditions
T = 10
prob = LinearStateSpaceProblem(A, B, u0, (0, T); C, observables_noise = D, syms = [:a, :b])
sol = solve(prob)
sol.z # simulated observables with observation noise</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11-element Vector{Vector{Float64}}:
 [0.3557859841705978, -0.77078182532573]
 [0.8409533469000959, 5.668991958635243]
 [0.8402123199802355, 6.78852128718408]
 [0.15850073983307233, 6.1515749096904235]
 [0.5779461136996231, 5.58468827107055]
 [0.6296070327766987, 6.6237523552408275]
 [0.5121306680948874, 4.890976538920878]
 [0.3025863973512676, 5.465293516480804]
 [0.6367825892700132, 4.702684303866116]
 [0.3833369173269628, 4.778157075402346]
 [0.5152753309268274, 4.311437256700383]</code></pre><p>Next, we will find the log likelihood of these simulated observables using <code>u0</code> as a prior and with the true parameters.</p><p>The new arguments we pass to the problem creation are <code>u0_prior_variance, u0_prior_mean,</code> and <code>observables</code>. The <code>u0</code> is ignored for the filtering problem, but must match the size. The <code>KalmanFilter()</code> argument to the <code>solve</code> is unnecessary since it can be selected automatically given the priors and observables.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The timing convention is such that <code>observables</code> are expected to match the predictions starting at the second time period. As the likelihood of the first element <code>u0</code> comes from a prior, the <code>observables</code> start at the next element, and hence the observables and noise sequences should be 1 less than the tspan.</p></div></div><pre><code class="language-julia hljs">observables = hcat(sol.z...)  # Observables required to be matrix.  Issue #55
observables = observables[:, 2:end] # see note above on likelihood and timing
noise = copy(sol.W) # save for later
u0_prior_mean = [0.0, 0.0]
# use covariance of distribution we drew from
u0_prior_var = cov(u0)

prob = LinearStateSpaceProblem(A, B, u0, (0, size(observables,2)); C, observables, observables_noise = D, syms = [:a, :b], u0_prior_var, u0_prior_mean)
sol = solve(prob, KalmanFilter())
# plot(sol) The `u` is the sequence of posterior means.
sol.logpdf</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-10.193869238630331</code></pre><p>Hence, the <code>logpdf</code> provides the log likelihood marginalizing out the latent noise variables.</p><p>As before, we can differentiate the kalman filter itself.</p><pre><code class="language-julia hljs">function kalman_likelihood(A, B, C, D, u0_prior_mean, u0_prior_var, observables)
    prob = LinearStateSpaceProblem(A, B, u0, (0, size(observables,2)); C, observables, observables_noise = D, syms = [:a, :b], u0_prior_var, u0_prior_mean)
    return solve(prob).logpdf
end
kalman_likelihood(A, B, C, D, u0_prior_mean, u0_prior_var, observables)
# Find the gradient wrt the A, B, C and priors variance.
gradient((A, B, C, u0_prior_var) -&gt; kalman_likelihood(A, B, C, D, u0_prior_mean, u0_prior_var, observables), A, B, C, u0_prior_var)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-58.06465200574262 -0.14701286581846165; -392.7844011668091 -8.96856476960024], [-6.358345245103165; -39.06194722390919;;], [14.758536627128432 0.4193390705195429; -2.424281483142579 0.09525220176111837], [-0.09755811653850696 3.3646678994539503; -3.258839754951192 -0.12524296651879752])</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Some gradients, such as those for <code>observables</code>, have not been implemented, so test carefully. This is a general theme with gradients and <code>Zygote.jl</code> in general. Your best friend in this process is the spectacular <a href="https://github.com/JuliaDiff/ChainRulesTestUtils.jl">ChainRulesTestUtils.jl</a> package. See <code>test_rrule</code> usage in the <a href="https://github.com/SciML/DifferenceEquations.jl/blob/main/test/linear_gradients.jl">linear unit tests</a>.</p></div></div><h2 id="Joint-Likelihood-with-Noise"><a class="docs-heading-anchor" href="#Joint-Likelihood-with-Noise">Joint Likelihood with Noise</a><a id="Joint-Likelihood-with-Noise-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-Likelihood-with-Noise" title="Permalink"></a></h2><p>A key application of these methods is to find the joint likelihood of the latent variables (i.e., the <code>noise</code>) and the model definition.</p><p>The actual calculation of the likelihood is trivial in that case, and just requires iteration of the linear system while accumulating the likelihood given the observation noise.</p><p>Crucially, the differentiability with respect to the high-dimensional noise vector enables gradient-based sampling and estimation methods that would otherwise be infeasible.</p><pre><code class="language-julia hljs">function joint_likelihood(A, B, C, D, u0, noise, observables)
    prob = LinearStateSpaceProblem(A, B, u0, (0, size(observables,2)); C, observables, observables_noise = D, noise)
    return solve(prob).logpdf
end
u0 = [0.0, 0.0]
joint_likelihood(A, B, C, D, u0, noise, observables)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-1480.9437401049217</code></pre><p>And as always, this can be differentiated with respect to the state-space matrices and the noise. Choosing a few parameters,</p><pre><code class="language-julia hljs">gradient((A, u0, noise) -&gt; joint_likelihood(A, B, C, D, u0, noise, observables), A, u0, noise)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([209.59241876045238 11.616239177375277; 1123.8589779378158 74.33574549926578], [419.6103631438355, 3347.6453006421193], [30.456778111485956 26.615998068080366 … 2.648052457171806 0.033833910623258454])</code></pre><h2 id="Composition-of-State-Space-Models-and-AD"><a class="docs-heading-anchor" href="#Composition-of-State-Space-Models-and-AD">Composition of State Space Models and AD</a><a id="Composition-of-State-Space-Models-and-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-of-State-Space-Models-and-AD" title="Permalink"></a></h2><p>While the above gradients have been with respect to the full state space objects <code>A, B</code>, etc. those themselves could be generated through a separate procedure and the whole object differentiated. For example, let&#39;s repeat the above examples where we generate the <code>A</code> matrix from some sort of deep parameters.</p><p>First, we will generate some observations with a <code>generate_model</code> proxy, which could be replaced with something more complicated but still differentiable</p><pre><code class="language-julia hljs">function generate_model(β)
    A = [β 6.2;
        0.0  0.2]
    B = Matrix([0.0  0.001]&#39;) # [0.0; 0.001;;] gives a zygote bug
    C = [0.09 0.67;
        1.00 0.00]
    D = [0.01, 0.01]
    return (;A,B,C,D)
end

function simulate_model(β, u0;T = 200)
    mod = generate_model(β)
    prob = LinearStateSpaceProblem(mod.A, mod.B, u0, (0, T); mod.C, observables_noise = mod.D)
    sol = solve(prob) # simulates
    observables = hcat(sol.z...)
    observables = observables[:, 2:end] # see note above on likelihood and timing
    return observables, sol.W
end

# Fix a &quot;pseudo-true&quot; and generate noise and observables
β = 0.95
u0 = [0.0, 0.0]
observables, noise = simulate_model(β, u0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([0.23459710314681284 -0.06201007497846065 … -0.06683773835177337 0.15255910231422928; 0.03689643886117321 0.09121301341283557 … -0.12385660767734548 -0.16698635493875724], [1.6630725892520406 1.0534583149878267 … -0.2547315525223579 -0.946697429355046])</code></pre><p>Next, we will evaluate the marginal likelihood using the kalman filter for a particular <code>β</code> value,</p><pre><code class="language-julia hljs">function kalman_model_likelihood(β, u0_prior_mean, u0_prior_var, observables)
    mod = generate_model(β) # generate model from structural parameters
    prob = LinearStateSpaceProblem(mod.A, mod.B, u0, (0, size(observables,2)); mod.C, observables,      observables_noise = mod.D, u0_prior_var, u0_prior_mean)
    return solve(prob).logpdf
end
u0_prior_mean = [0.0, 0.0]
u0_prior_var = [1e-10 0.0;
                0.0 1e-10]  # starting with degenerate prior
kalman_model_likelihood(β, u0_prior_mean, u0_prior_var, observables)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">340.7465239685115</code></pre><p>Given the observation error, we would not expect the pseudo-true to exactly maximize the log likelihood. To show this, we can optimize it using the Optim package, specifically using a gradient-based optimization routine</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
# Create a function to minimize only of β and use Zygote based gradients
kalman_objective(β,p) = -kalman_model_likelihood(β, u0_prior_mean, u0_prior_var, observables)
kalman_objective(0.95, nothing)
gradient(β -&gt;kalman_objective(β, nothing),β) # Verifying it can be differentiated


optf = OptimizationFunction(kalman_objective, Optimization.AutoZygote())
β0 = [0.91] # start off of the pseudotrue
optprob = OptimizationProblem(optf, β0)
optsol = solve(optprob,LBFGS())  # reverse-mode AD is overkill here</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 1-element Vector{Float64}:
 0.9769832662086287</code></pre><p>In this way, this package composes with others such as <a href="https://github.com/HighDimensionalEconLab/DifferentiableStateSpaceModels.jl">DifferentiableStateSpaceModels.jl</a> which takes a set of structural parameters and an expected difference equation to generate a state-space model.</p><p>Similarly, we can find the joint likelihood for a particular <code>β</code> value and noise. Here we will add in prior. Some form of prior or regularization is generally necessary for these sorts of nonlinear models.</p><pre><code class="language-julia hljs">function joint_model_posterior(β, u0, noise, observables, noise_prior, β_prior)
    mod = generate_model(β) # generate model from structural parameters
    prob = LinearStateSpaceProblem(mod.A, mod.B, u0, (0, size(observables,2)); mod.C, observables,      observables_noise = mod.D, noise)
    return solve(prob).logpdf + sum(logpdf.(noise_prior, noise)) + logpdf(β_prior, β) # posterior
end
u0 = [0.0, 0.0]
noise_prior = Normal(0.0, 1.0)
β_prior = Normal(β, 0.03) # prior local to the true value
joint_model_posterior(β, u0, noise, observables, noise_prior, β_prior)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">67.49416779134839</code></pre><p>Which we can turn into a differentiable objective by adding in a prior on the noise</p><pre><code class="language-julia hljs">joint_model_objective(x, p) = -joint_model_posterior(x[1], u0, Matrix(x[2:end]&#39;), observables, noise_prior, β_prior) # extract noise and parameeter from vector
x0 = vcat([0.95], noise[1,:])  # starting at the true noise
joint_model_objective(x0, nothing)
gradient(x -&gt;joint_model_objective(x, nothing),x0) # Verifying it can be differentiated

# optimize
optf = OptimizationFunction(joint_model_objective, Optimization.AutoZygote())
optprob = OptimizationProblem(optf, x0)
optsol = solve(optprob,LBFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 201-element Vector{Float64}:
  0.9881924802117322
  0.20996380696904232
  0.14722739738355123
  0.18757591776171942
  0.22405331718054167
  0.27289353336626226
  0.21730140599345807
  0.23626104860016406
  0.1903066224467735
  0.20747802439932184
  ⋮
 -0.3813039433954503
 -0.2972670647236484
 -0.21076657904321236
 -0.25237758352832107
 -0.1684207390822031
 -0.0975104502751503
 -0.14575416978286762
 -0.0737716464040692
  0.010447779451547155</code></pre><p>This &quot;solves&quot; the problem relatively quickly, despite the high-dimensionality. However, from a statistics perspective note that this last optimization process does not do especially well in recovering the pseudotrue if you increase the prior variance on the <code>β</code> parameter. Maximizing the posterior is usually the wrong thing to do in high-dimensions because the mode is not a typical set.</p><h2 id="Caveats-on-Gradients-and-Performance"><a class="docs-heading-anchor" href="#Caveats-on-Gradients-and-Performance">Caveats on Gradients and Performance</a><a id="Caveats-on-Gradients-and-Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Caveats-on-Gradients-and-Performance" title="Permalink"></a></h2><p>A few notes on performance and gradients:</p><ol><li>As this is using reverse-mode AD it will be efficient for fairly large systems as long as the ultimate value of your differentiable program. With a little extra work and unit tests, it could support structured matrices/etc. as well.</li><li>Getting to much higher scales, where the <code>A,B,C,D</code> are so large that matrix-free operators are necessary, is feasible but will require generalizing those to LinearOperators. This would be reasonably easy for joint likelihood and feasible but possible for the Kalman filter.</li><li>At this point, there is no support for forward-mode auto-differentiation. For smaller systems with a kalman filter, this should dominate the alternatives, and efficient forward-mode AD rules for the kalman filter exist (see the supplementary materials in the <a href="https://github.com/HighDimensionalEconLab/DifferentiableStateSpaceModels.jl">Differentiable State Space Models</a> paper). However, it would be a significant amount of work to add end-to-end support and fulfill standard SciML interfaces, and perhaps waiting for <a href="https://enzyme.mit.edu/julia/">Enzyme</a> or similar AD systems that provide both forward/reverse/mixed mode makes sense.</li><li>Forward-mode AD is likely inappropriate for the joint-likelihood based models, since the dimensionality of the noise is always large.</li><li>The gradient rules are written using <a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules.jl</a> so in theory they will work with any supporting AD. In practice, though, Zygote is the most tested, and other systems have inconsistent support for Julia at this time.</li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« DifferenceEquations.jl: Discrete-Time State Space Solution Methods</a><a class="docs-footer-nextpage" href="../quadratic_state_space_examples/">Quadratic State Space Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.0 on <span class="colophon-date" title="Monday 2 October 2023 00:52">Monday 2 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
