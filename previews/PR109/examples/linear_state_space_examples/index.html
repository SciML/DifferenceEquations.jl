<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear State Space Examples · DifferenceEquations.jl</title><meta name="title" content="Linear State Space Examples · DifferenceEquations.jl"/><meta property="og:title" content="Linear State Space Examples · DifferenceEquations.jl"/><meta property="twitter:title" content="Linear State Space Examples · DifferenceEquations.jl"/><meta name="description" content="Documentation for DifferenceEquations.jl."/><meta property="og:description" content="Documentation for DifferenceEquations.jl."/><meta property="twitter:description" content="Documentation for DifferenceEquations.jl."/><meta property="og:url" content="https://DifferenceEquations.sciml.ai/stable/examples/linear_state_space_examples/"/><meta property="twitter:url" content="https://DifferenceEquations.sciml.ai/stable/examples/linear_state_space_examples/"/><link rel="canonical" href="https://DifferenceEquations.sciml.ai/stable/examples/linear_state_space_examples/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DifferenceEquations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DifferenceEquations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">DifferenceEquations.jl: Discrete-Time State Space Solution Methods</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Linear State Space Examples</a><ul class="internal"><li><a class="tocitem" href="#Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model"><span>Simulating a Linear (and Time-Invariant) State Space Model</span></a></li><li><a class="tocitem" href="#Simulating-Ensembles-and-Fixing-Noise"><span>Simulating Ensembles and Fixing Noise</span></a></li><li><a class="tocitem" href="#Observables-and-Marginal-Likelihood-using-a-Kalman-Filter"><span>Observables and Marginal Likelihood using a Kalman Filter</span></a></li><li><a class="tocitem" href="#Joint-Likelihood-with-Noise"><span>Joint Likelihood with Noise</span></a></li><li><a class="tocitem" href="#Composition-of-State-Space-Models-and-AD"><span>Composition of State Space Models and AD</span></a></li><li><a class="tocitem" href="#Caveats-on-Gradients-and-Performance"><span>Caveats on Gradients and Performance</span></a></li></ul></li><li><a class="tocitem" href="../quadratic_state_space_examples/">Quadratic State Space Examples</a></li><li><a class="tocitem" href="../general_state_space_examples/">General State Space Examples</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Linear State Space Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear State Space Examples</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/DifferenceEquations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/DifferenceEquations.jl/blob/main/docs/src/examples/linear_state_space_examples.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-State-Space-Examples"><a class="docs-heading-anchor" href="#Linear-State-Space-Examples">Linear State Space Examples</a><a id="Linear-State-Space-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-State-Space-Examples" title="Permalink"></a></h1><p>This tutorial describes the support for linear and linear gaussian state space models.</p><p>At this point, the package only supports linear time-invariant models without a separate <code>p</code> vector. The canonical form of the linear model is</p><p class="math-container">\[u_{n+1} = A u_n + B w_{n+1}\]</p><p>with</p><p class="math-container">\[z_n = C u_n +  v_n\]</p><p>and optionally <span>$v_n \sim N(0, D)$</span> and <span>$w_{n+1} \sim N(0,I)$</span>.  If you pass noise into the solver, it no longer needs to be Gaussian. More generally, support could be added for <span>$u_{n+1} = A(p,n) u_n + B(p,n) w_{n+1}$</span> where <span>$p$</span> is a vector of differentiable parameters, and the <span>$A$</span> and <span>$B$</span> are potentially matrix-free operators.</p><h2 id="Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model"><a class="docs-heading-anchor" href="#Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model">Simulating a Linear (and Time-Invariant) State Space Model</a><a id="Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-a-Linear-(and-Time-Invariant)-State-Space-Model" title="Permalink"></a></h2><p>Creating a <code>LinearStateSpaceProblem</code> and simulating it for a simple, linear equation.</p><pre><code class="language-julia hljs">using DifferenceEquations, LinearAlgebra, Distributions, Random, Plots, DataFrames, Zygote
A = [0.95 6.2;
     0.0  0.2]
B = [0.0; 0.01;;] # matrix
C = [0.09 0.67;
     1.00 0.00]
D = [0.1, 0.1] # diagonal observation noise
u0 = zeros(2)
T = 10

prob = LinearStateSpaceProblem(A, B, u0, (0, T); C, observables_noise = D, syms = [:a, :b])
sol = solve(prob)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
Interpolation: Piecewise constant interpolation
t: 0:10
u: 11-element Vector{Vector{Float64}}:
 [0.0, 0.0]
 [0.0, 0.014949917533966035]
 [0.09268948871058942, -0.0006575727730867894]
 [0.08397806308192185, 0.013192460420252749]
 [0.1615724145333928, -0.0014408968217409766]
 [0.1445602335119291, -0.010825646365913507]
 [0.07021321436766888, -0.00545961164780032]
 [0.03285296143292345, 0.009162219923490492]
 [0.08801607688691832, 0.015544182796745923]
 [0.17998920638239713, 0.00690927385105727]
 [0.21382724393983235, -0.001413634505199068]</code></pre><p>The <code>u</code> vector of the simulated solution can be plotted using the standard recipes, including the use of the optional <code>syms</code>. See the <a href="https://diffeq.sciml.ai/latest/basics/plot/">SciML docs</a> for more options.</p><pre><code class="language-julia hljs">plot(sol)</code></pre><img src="c3b8f712.svg" alt="Example block output"/><p>By default, the solution provides an interface to access the simulated <code>u</code>.  That is, <code>sol.u[...] = sol[...]</code>,</p><pre><code class="language-julia hljs">sol[2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.0
 0.014949917533966035</code></pre><p>Or to get the first element of the last step</p><pre><code class="language-julia hljs">sol[end][1] #first element of last step</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.21382724393983235</code></pre><p>Finally, to extract the full vector</p><pre><code class="language-julia hljs">@show sol[2,:];  # whole second vector</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11-element Vector{Float64}:
  0.0
  0.014949917533966035
 -0.0006575727730867894
  0.013192460420252749
 -0.0014408968217409766
 -0.010825646365913507
 -0.00545961164780032
  0.009162219923490492
  0.015544182796745923
  0.00690927385105727
 -0.001413634505199068</code></pre><p>The results for all of <code>sol.u</code> can be loaded in a dataframe, where the column names will be the (optionally) provided symbols.</p><pre><code class="language-julia hljs">df = DataFrame(sol)</code></pre><div><div style = "float: left;"><span>11×3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">timestamp</th><th style = "text-align: left;">a</th><th style = "text-align: left;">b</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">0.0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">0.0149499</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">2</td><td style = "text-align: right;">0.0926895</td><td style = "text-align: right;">-0.000657573</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">3</td><td style = "text-align: right;">0.0839781</td><td style = "text-align: right;">0.0131925</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">4</td><td style = "text-align: right;">0.161572</td><td style = "text-align: right;">-0.0014409</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">6</td><td style = "text-align: right;">5</td><td style = "text-align: right;">0.14456</td><td style = "text-align: right;">-0.0108256</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">7</td><td style = "text-align: right;">6</td><td style = "text-align: right;">0.0702132</td><td style = "text-align: right;">-0.00545961</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">8</td><td style = "text-align: right;">7</td><td style = "text-align: right;">0.032853</td><td style = "text-align: right;">0.00916222</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">9</td><td style = "text-align: right;">8</td><td style = "text-align: right;">0.0880161</td><td style = "text-align: right;">0.0155442</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">10</td><td style = "text-align: right;">9</td><td style = "text-align: right;">0.179989</td><td style = "text-align: right;">0.00690927</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">11</td><td style = "text-align: right;">10</td><td style = "text-align: right;">0.213827</td><td style = "text-align: right;">-0.00141363</td></tr></tbody></table></div><p>Other results, such as the simulated noise and observables, can be extracted from the solution</p><pre><code class="language-julia hljs">sol.z # observables</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11-element Vector{Vector{Float64}}:
 [0.3250471000129226, -0.48650236227176497]
 [-0.05056535249131665, 0.14259786215117057]
 [0.11260178581449515, -0.05508417476983514]
 [0.16177299981893545, -0.004445350224361988]
 [0.32814822082628614, 0.3945488778004209]
 [-0.21638701206219937, -0.03350316138101683]
 [0.2643035595109182, 0.10524509920730751]
 [-0.012017305665378742, 0.20581822374991016]
 [0.07180699577369948, 0.1555260065447733]
 [-0.2229408271714284, 0.36193025599200945]
 [0.3412008284141197, 0.0930270836899399]</code></pre><pre><code class="language-julia hljs">sol.W # Simulated Noise</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×10 Matrix{Float64}:
 1.49499  -0.364756  1.3324  -0.407939  …  1.37117  0.380044  -0.279549</code></pre><p>We can also solve the model by passing in fixed noise, which will be useful for joint likelihoods. First, let&#39;s extract the noise from the previous solution, then rerun the simulation but with a different initial value</p><pre><code class="language-julia hljs">noise = sol.W
u0_2 = [0.1, 0.0]
prob2 = LinearStateSpaceProblem(A, B, u0_2, (0, T); C, observables_noise = D, syms = [:a, :b], noise)
sol2 = solve(prob2)
plot(sol2)</code></pre><img src="5ee66103.svg" alt="Example block output"/><p>To construct an IRF we can take the model and perturb just the first element of the noise,</p><pre><code class="language-julia hljs">function irf(A, B, C, T = 20)
    noise = Matrix([1.0; zeros(T-1)]&#39;)
    problem = LinearStateSpaceProblem(A, B, zeros(2), (0, T); C, noise, syms = [:a, :b])
    return solve(problem)
end
plot(irf(A, B, C))</code></pre><img src="ef77dfa1.svg" alt="Example block output"/><p>Let&#39;s find the 2nd observable at the end of the IRF.</p><pre><code class="language-julia hljs">function last_observable_irf(A, B, C)
    sol = irf(A, B, C)
    return sol.z[end][2]  # return 2nd argument of last observable
end
last_observable_irf(A, B, C)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.03119456447624772</code></pre><p>But everything in this package is differentiable. Let&#39;s differentiate the observable of the IRF with respect to all the parameters using <code>Zygote.jl</code>,</p><pre><code class="language-julia hljs">gradient(last_observable_irf, A, B, C)  # calculates gradient wrt all arguments</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([0.5822985368900442 0.0050313813671367304; 4.469834483178462 0.041592752634585235], [0.37735360253530714; 3.119456447624773;;], [0.0 0.0; 0.03119456447624772 5.242880000000006e-16])</code></pre><p>Gradients of other model elements (e.g. <code>.u</code>) are also possible. With this in mind, let&#39;s find the gradient of the mean of the 1st element of the IRF of the solution with respect to a particular noise vector.</p><pre><code class="language-julia hljs">function mean_u_1(A, B, C, noise, u0, T)
    problem = LinearStateSpaceProblem(A, B, u0, (0, T); noise, syms = [:a, :b])
    sol = solve(problem)
    u = sol.u # see issue #75 workaround
    # can have nontrivial functions and even non-mutating loops
    return mean( u[i][1] for i in 1:T)
end
u0 = [0.0, 0.0]
noise = sol.W # from simulation above
mean_u_1(A, B, C, noise, u0, T)
# dropping a few arguments from derivative
gradient((noise, u0)-&gt; mean_u_1(A, B, C, noise, u0, T), noise, u0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([0.05079876954953124 0.045314515146875 … 0.0 0.0], [0.702526121523242, 5.600882710405467])</code></pre><h2 id="Simulating-Ensembles-and-Fixing-Noise"><a class="docs-heading-anchor" href="#Simulating-Ensembles-and-Fixing-Noise">Simulating Ensembles and Fixing Noise</a><a id="Simulating-Ensembles-and-Fixing-Noise-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-Ensembles-and-Fixing-Noise" title="Permalink"></a></h2><p>If you pass in a distribution for the initial condition, it will draw an initial condition. Below, we will simulate from a deterministic evolution equation, without any observation noise.</p><pre><code class="language-julia hljs">using Distributions, DiffEqBase
u0 = MvNormal([1.0 0.1; 0.1 1.0])  # mean zero initial conditions
prob = LinearStateSpaceProblem(A, nothing, u0, (0, T); C)
sol = solve(prob)
plot(sol)</code></pre><img src="031faee7.svg" alt="Example block output"/><p>With this, we can simulate an ensemble of solutions from different initial conditions (and we will turn back on the noise). The <code>EnsembleSummary</code> calculates a set of quantiles by default.</p><pre><code class="language-julia hljs">T = 10
trajectories = 50
prob = LinearStateSpaceProblem(A, B, u0, (0, T); C)
sol = solve(EnsembleProblem(prob), DirectIteration(), EnsembleThreads(); trajectories)
summ = EnsembleSummary(sol)  #calculate summarize statistics from the
plot(summ)  # shows quantiles by default</code></pre><img src="22ce6605.svg" alt="Example block output"/><h2 id="Observables-and-Marginal-Likelihood-using-a-Kalman-Filter"><a class="docs-heading-anchor" href="#Observables-and-Marginal-Likelihood-using-a-Kalman-Filter">Observables and Marginal Likelihood using a Kalman Filter</a><a id="Observables-and-Marginal-Likelihood-using-a-Kalman-Filter-1"></a><a class="docs-heading-anchor-permalink" href="#Observables-and-Marginal-Likelihood-using-a-Kalman-Filter" title="Permalink"></a></h2><p>If you provide <code>observables</code> and provide a distribution for the <code>observables_noise</code> then the model can provide a calculation of the likelihood.</p><p>The simplest case is if you use a gaussian prior and have gaussian observation noise. First, let&#39;s simulate some data with included observation noise. If passing in a matrix or vector, the <code>observables_noise</code> argument is intended to be the cholesky of the covariance matrix. At this point, only diagonal observation noise is allowed.</p><pre><code class="language-julia hljs">u0 = MvNormal([1.0 0.1; 0.1 1.0])  # draw from mean zero initial conditions
T = 10
prob = LinearStateSpaceProblem(A, B, u0, (0, T); C, observables_noise = D, syms = [:a, :b])
sol = solve(prob)
sol.z # simulated observables with observation noise</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11-element Vector{Vector{Float64}}:
 [-0.6164468598579631, -0.43040813944853773]
 [-0.6544861635353549, -5.814236679697214]
 [-0.8282928533492595, -6.40687512936875]
 [-0.30600086491748363, -6.172049227846021]
 [-0.8364330092001846, -5.905957409566316]
 [-0.5967139577371271, -6.042465989620388]
 [-0.0709358094873006, -5.120936539542132]
 [-0.13997719303572104, -5.840071331651632]
 [-0.9934269878261337, -5.027166635113764]
 [-0.12817001002322437, -4.675663115301252]
 [-0.8074036796754438, -5.234904006117631]</code></pre><p>Next, we will find the log likelihood of these simulated observables using <code>u0</code> as a prior and with the true parameters.</p><p>The new arguments we pass to the problem creation are <code>u0_prior_variance, u0_prior_mean,</code> and <code>observables</code>. The <code>u0</code> is ignored for the filtering problem, but must match the size. The <code>KalmanFilter()</code> argument to the <code>solve</code> is unnecessary since it can be selected automatically given the priors and observables.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The timing convention is such that <code>observables</code> are expected to match the predictions starting at the second time period. As the likelihood of the first element <code>u0</code> comes from a prior, the <code>observables</code> start at the next element, and hence the observables and noise sequences should be 1 less than the tspan.</p></div></div><pre><code class="language-julia hljs">observables = hcat(sol.z...)  # Observables required to be matrix.  Issue #55
observables = observables[:, 2:end] # see note above on likelihood and timing
noise = copy(sol.W) # save for later
u0_prior_mean = [0.0, 0.0]
# use covariance of distribution we drew from
u0_prior_var = cov(u0)

prob = LinearStateSpaceProblem(A, B, u0, (0, size(observables,2)); C, observables, observables_noise = D, syms = [:a, :b], u0_prior_var, u0_prior_mean)
sol = solve(prob, KalmanFilter())
# plot(sol) The `u` is the sequence of posterior means.
sol.logpdf</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-11.01606896478929</code></pre><p>Hence, the <code>logpdf</code> provides the log likelihood marginalizing out the latent noise variables.</p><p>As before, we can differentiate the kalman filter itself.</p><pre><code class="language-julia hljs">function kalman_likelihood(A, B, C, D, u0_prior_mean, u0_prior_var, observables)
    prob = LinearStateSpaceProblem(A, B, u0, (0, size(observables,2)); C, observables, observables_noise = D, syms = [:a, :b], u0_prior_var, u0_prior_mean)
    return solve(prob).logpdf
end
kalman_likelihood(A, B, C, D, u0_prior_mean, u0_prior_var, observables)
# Find the gradient wrt the A, B, C and priors variance.
gradient((A, B, C, u0_prior_var) -&gt; kalman_likelihood(A, B, C, D, u0_prior_mean, u0_prior_var, observables), A, B, C, u0_prior_var)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([158.0374273397436 -0.014608980098392538; 1344.8714034255295 -3.9480006927919717], [2.7218435278170157; 21.290761022964997;;], [6.666146045797596 0.08188236184873865; -0.8456715646370867 0.16257277709167717], [-0.08500982112925845 4.156564119313; -4.007795135781049 -0.13174950137712926])</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Some gradients, such as those for <code>observables</code>, have not been implemented, so test carefully. This is a general theme with gradients and <code>Zygote.jl</code> in general. Your best friend in this process is the spectacular <a href="https://github.com/JuliaDiff/ChainRulesTestUtils.jl">ChainRulesTestUtils.jl</a> package. See <code>test_rrule</code> usage in the <a href="https://github.com/SciML/DifferenceEquations.jl/blob/main/test/linear_gradients.jl">linear unit tests</a>.</p></div></div><h2 id="Joint-Likelihood-with-Noise"><a class="docs-heading-anchor" href="#Joint-Likelihood-with-Noise">Joint Likelihood with Noise</a><a id="Joint-Likelihood-with-Noise-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-Likelihood-with-Noise" title="Permalink"></a></h2><p>A key application of these methods is to find the joint likelihood of the latent variables (i.e., the <code>noise</code>) and the model definition.</p><p>The actual calculation of the likelihood is trivial in that case, and just requires iteration of the linear system while accumulating the likelihood given the observation noise.</p><p>Crucially, the differentiability with respect to the high-dimensional noise vector enables gradient-based sampling and estimation methods that would otherwise be infeasible.</p><pre><code class="language-julia hljs">function joint_likelihood(A, B, C, D, u0, noise, observables)
    prob = LinearStateSpaceProblem(A, B, u0, (0, size(observables,2)); C, observables, observables_noise = D, noise)
    return solve(prob).logpdf
end
u0 = [0.0, 0.0]
joint_likelihood(A, B, C, D, u0, noise, observables)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-1654.6020467981823</code></pre><p>And as always, this can be differentiated with respect to the state-space matrices and the noise. Choosing a few parameters,</p><pre><code class="language-julia hljs">gradient((A, u0, noise) -&gt; joint_likelihood(A, B, C, D, u0, noise, observables), A, u0, noise)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-276.4025330209832 -7.214654807620111; -1762.7921439930785 -63.52435697592466], [-443.168877074868, -3540.3268713468083], [-32.40334157448875 -28.73757521002897 … -3.218508310417373 -0.053458411508805405])</code></pre><h2 id="Composition-of-State-Space-Models-and-AD"><a class="docs-heading-anchor" href="#Composition-of-State-Space-Models-and-AD">Composition of State Space Models and AD</a><a id="Composition-of-State-Space-Models-and-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-of-State-Space-Models-and-AD" title="Permalink"></a></h2><p>While the above gradients have been with respect to the full state space objects <code>A, B</code>, etc. those themselves could be generated through a separate procedure and the whole object differentiated. For example, let&#39;s repeat the above examples where we generate the <code>A</code> matrix from some sort of deep parameters.</p><p>First, we will generate some observations with a <code>generate_model</code> proxy, which could be replaced with something more complicated but still differentiable</p><pre><code class="language-julia hljs">function generate_model(β)
    A = [β 6.2;
        0.0  0.2]
    B = Matrix([0.0  0.001]&#39;) # [0.0; 0.001;;] gives a zygote bug
    C = [0.09 0.67;
        1.00 0.00]
    D = [0.01, 0.01]
    return (;A,B,C,D)
end

function simulate_model(β, u0;T = 200)
    mod = generate_model(β)
    prob = LinearStateSpaceProblem(mod.A, mod.B, u0, (0, T); mod.C, observables_noise = mod.D)
    sol = solve(prob) # simulates
    observables = hcat(sol.z...)
    observables = observables[:, 2:end] # see note above on likelihood and timing
    return observables, sol.W
end

# Fix a &quot;pseudo-true&quot; and generate noise and observables
β = 0.95
u0 = [0.0, 0.0]
observables, noise = simulate_model(β, u0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-0.05187165237163429 0.009372622623846423 … -0.1011705271773321 0.13798573046902393; -0.16734162305254277 -0.08562079597123304 … 0.040335454981627716 0.00020018190054945165], [-0.43815692079595525 -0.3244098957083064 … 2.7051763117204937 1.4039937803659732])</code></pre><p>Next, we will evaluate the marginal likelihood using the kalman filter for a particular <code>β</code> value,</p><pre><code class="language-julia hljs">function kalman_model_likelihood(β, u0_prior_mean, u0_prior_var, observables)
    mod = generate_model(β) # generate model from structural parameters
    prob = LinearStateSpaceProblem(mod.A, mod.B, u0, (0, size(observables,2)); mod.C, observables,      observables_noise = mod.D, u0_prior_var, u0_prior_mean)
    return solve(prob).logpdf
end
u0_prior_mean = [0.0, 0.0]
u0_prior_var = [1e-10 0.0;
                0.0 1e-10]  # starting with degenerate prior
kalman_model_likelihood(β, u0_prior_mean, u0_prior_var, observables)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">359.1785933660443</code></pre><p>Given the observation error, we would not expect the pseudo-true to exactly maximize the log likelihood. To show this, we can optimize it using the Optim package, specifically using a gradient-based optimization routine</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
# Create a function to minimize only of β and use Zygote based gradients
kalman_objective(β,p) = -kalman_model_likelihood(β, u0_prior_mean, u0_prior_var, observables)
kalman_objective(0.95, nothing)
gradient(β -&gt;kalman_objective(β, nothing),β) # Verifying it can be differentiated


optf = OptimizationFunction(kalman_objective, Optimization.AutoZygote())
β0 = [0.91] # start off of the pseudotrue
optprob = OptimizationProblem(optf, β0)
optsol = solve(optprob,LBFGS())  # reverse-mode AD is overkill here</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 1-element Vector{Float64}:
 -0.12761136851188845</code></pre><p>In this way, this package composes with others such as <a href="https://github.com/HighDimensionalEconLab/DifferentiableStateSpaceModels.jl">DifferentiableStateSpaceModels.jl</a> which takes a set of structural parameters and an expected difference equation to generate a state-space model.</p><p>Similarly, we can find the joint likelihood for a particular <code>β</code> value and noise. Here we will add in prior. Some form of prior or regularization is generally necessary for these sorts of nonlinear models.</p><pre><code class="language-julia hljs">function joint_model_posterior(β, u0, noise, observables, noise_prior, β_prior)
    mod = generate_model(β) # generate model from structural parameters
    prob = LinearStateSpaceProblem(mod.A, mod.B, u0, (0, size(observables,2)); mod.C, observables,      observables_noise = mod.D, noise)
    return solve(prob).logpdf + sum(logpdf.(noise_prior, noise)) + logpdf(β_prior, β) # posterior
end
u0 = [0.0, 0.0]
noise_prior = Normal(0.0, 1.0)
β_prior = Normal(β, 0.03) # prior local to the true value
joint_model_posterior(β, u0, noise, observables, noise_prior, β_prior)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">78.1472649150981</code></pre><p>Which we can turn into a differentiable objective by adding in a prior on the noise</p><pre><code class="language-julia hljs">joint_model_objective(x, p) = -joint_model_posterior(x[1], u0, Matrix(x[2:end]&#39;), observables, noise_prior, β_prior) # extract noise and parameeter from vector
x0 = vcat([0.95], noise[1,:])  # starting at the true noise
joint_model_objective(x0, nothing)
gradient(x -&gt;joint_model_objective(x, nothing),x0) # Verifying it can be differentiated

# optimize
optf = OptimizationFunction(joint_model_objective, Optimization.AutoZygote())
optprob = OptimizationProblem(optf, x0)
optsol = solve(optprob,LBFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 201-element Vector{Float64}:
  0.9617332732220298
 -0.08184098489390636
 -0.0166739836216118
  0.03195643241910501
  0.17574973488962287
  0.2784739577354817
  0.23322631360014714
  0.24429715982005282
  0.21230504666059732
  0.22442424272931558
  ⋮
  0.00838512225883158
  0.04085692406003387
  0.1763949543856435
  0.17135216218519952
  0.10765672399441788
  0.07946892904825181
  0.039918897666753196
  0.008346642138404038
  0.009296428514020495</code></pre><p>This &quot;solves&quot; the problem relatively quickly, despite the high-dimensionality. However, from a statistics perspective note that this last optimization process does not do especially well in recovering the pseudotrue if you increase the prior variance on the <code>β</code> parameter. Maximizing the posterior is usually the wrong thing to do in high-dimensions because the mode is not a typical set.</p><h2 id="Caveats-on-Gradients-and-Performance"><a class="docs-heading-anchor" href="#Caveats-on-Gradients-and-Performance">Caveats on Gradients and Performance</a><a id="Caveats-on-Gradients-and-Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Caveats-on-Gradients-and-Performance" title="Permalink"></a></h2><p>A few notes on performance and gradients:</p><ol><li>As this is using reverse-mode AD it will be efficient for fairly large systems as long as the ultimate value of your differentiable program. With a little extra work and unit tests, it could support structured matrices/etc. as well.</li><li>Getting to much higher scales, where the <code>A,B,C,D</code> are so large that matrix-free operators are necessary, is feasible but will require generalizing those to LinearOperators. This would be reasonably easy for joint likelihood and feasible but possible for the Kalman filter.</li><li>At this point, there is no support for forward-mode auto-differentiation. For smaller systems with a kalman filter, this should dominate the alternatives, and efficient forward-mode AD rules for the kalman filter exist (see the supplementary materials in the <a href="https://github.com/HighDimensionalEconLab/DifferentiableStateSpaceModels.jl">Differentiable State Space Models</a> paper). However, it would be a significant amount of work to add end-to-end support and fulfill standard SciML interfaces, and perhaps waiting for <a href="https://enzyme.mit.edu/julia/">Enzyme</a> or similar AD systems that provide both forward/reverse/mixed mode makes sense.</li><li>Forward-mode AD is likely inappropriate for the joint-likelihood based models, since the dimensionality of the noise is always large.</li><li>The gradient rules are written using <a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules.jl</a> so in theory they will work with any supporting AD. In practice, though, Zygote is the most tested, and other systems have inconsistent support for Julia at this time.</li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« DifferenceEquations.jl: Discrete-Time State Space Solution Methods</a><a class="docs-footer-nextpage" href="../quadratic_state_space_examples/">Quadratic State Space Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.0 on <span class="colophon-date" title="Monday 2 October 2023 01:04">Monday 2 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
